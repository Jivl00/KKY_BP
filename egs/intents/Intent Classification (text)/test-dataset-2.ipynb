{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitt/miniconda3/envs/net/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No sentence-transformers model found with name /Users/kitt/.cache/torch/sentence_transformers/fav-kky_FERNET-C5. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/kitt/.cache/torch/sentence_transformers/fav-kky_FERNET-C5 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NET_PARAMS = {\n",
    "    'hidden_shape': [50, 75],\n",
    "    'learning_rate': 0.03,\n",
    "    'batch_size': 64,\n",
    "    'transfer': 'sigmoid',\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, inp_shape, hidden_shape, out_shape, transfer=NET_PARAMS['transfer']):\n",
    "        super(Net, self).__init__()\n",
    "        self.inp_shape = inp_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "        self.transfer = torch.sigmoid if transfer == 'sigmoid' else torch.tanh\n",
    "        self.input_layer = nn.Linear(self.inp_shape, self.hidden_shape[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.hidden_shape[hi-1], self.hidden_shape[hi]) for hi in range(1, len(self.hidden_shape))])\n",
    "        self.output_layer = nn.Linear(self.hidden_shape[-1], self.out_shape)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transfer(self.input_layer(x))\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.transfer(layer(x))\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def new_m(self, m):\n",
    "        self.output_layer = nn.Linear(self.hidden_shape[-1], m)\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.text2vec = SentenceTransformer('fav-kky/FERNET-C5')\n",
    "\n",
    "        # Net\n",
    "        self.net = None\n",
    "\n",
    "        # Labels\n",
    "        self.target2label = None\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def encode_input(self, text):\n",
    "        return self.text2vec.encode(text)\n",
    "    \n",
    "    def decode_output(self, vec):\n",
    "        return self.target2label[torch.argmax(vec).item()]\n",
    "\n",
    "    def init_net(self, dataset, print_summary=True):\n",
    "        self.net = Net(dataset['train'].n, NET_PARAMS['hidden_shape'], dataset['train'].m)\n",
    "        self.target2label = dataset['train'].target2label\n",
    "        self.optimizer = torch.optim.SGD(self.net.parameters(), lr=NET_PARAMS['learning_rate'])\n",
    "        \n",
    "        if print_summary:\n",
    "            summary(self.net, (1, dataset['train'].n))\n",
    "\n",
    "    def reinit_net(self, dataset, keep_weights=True, print_summary=True):\n",
    "        self.net.new_m(dataset['train'].m)\n",
    "        self.target2label = dataset['train'].target2label\n",
    "        self.optimizer = torch.optim.SGD(self.net.parameters(), lr=NET_PARAMS['learning_rate'])\n",
    "        \n",
    "        if print_summary:\n",
    "            summary(self.net, (1, dataset['train'].n))\n",
    "\n",
    "    def fit(self, trainloader, devloader=None, epochs=50, verbose=True, save=True):\n",
    "        train_loss_list = []\n",
    "        dev_loss_list = []\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] *= 0.75\n",
    "                    print(f'Epoch {epoch}: Learning rate changed to {g[\"lr\"]}')\n",
    "            \n",
    "            # Set model to train configuration\n",
    "            self.net.train()\n",
    "            epoch_train_loss_list = []\n",
    "            for x, y_true, _, _ in trainloader:\n",
    "                # Clear gradient\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Make a prediction\n",
    "                y_pred = self.net(x)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(y_pred, y_true)\n",
    "\n",
    "                # Calculate gradients of parameters\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_train_loss_list.append(loss.data)\n",
    "\n",
    "            # Set model to eval configuration\n",
    "            self.net.eval()\n",
    "            epoch_dev_loss_list = []\n",
    "            for x, y_true, _, _ in devloader:\n",
    "                \n",
    "                y_pred = self.net(x)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(y_pred, y_true)\n",
    "\n",
    "                epoch_dev_loss_list.append(loss.data)\n",
    "            \n",
    "            mean_train_loss = np.mean([l.item() for l in epoch_train_loss_list])\n",
    "            mean_dev_loss = np.mean([l.item() for l in epoch_dev_loss_list])\n",
    "            \n",
    "            train_loss_list.append(mean_train_loss)\n",
    "            dev_loss_list.append(mean_dev_loss)\n",
    "            \n",
    "            if verbose > 0 and epoch % 50 == 0:\n",
    "                print(f'epoch {epoch}, train loss {mean_train_loss}, dev loss {mean_dev_loss}')\n",
    "\n",
    "            if len(dev_loss_list) > NET_PARAMS['patience'] and all([dl < mean_dev_loss for dl in dev_loss_list[-NET_PARAMS['patience']:-1]]):\n",
    "                print(f'Early stopping, dev_loss tail: {dev_loss_list[-NET_PARAMS[\"patience\"]:-1]}')\n",
    "                break\n",
    "\n",
    "\n",
    "        print(f'Final train loss: {train_loss_list[-1].item()}, dev loss: {dev_loss_list[-1].item()}')\n",
    "\n",
    "        if save:\n",
    "            torch.save(self.net.state_dict(), 'model-test')\n",
    "\n",
    "        return train_loss_list, dev_loss_list\n",
    "\n",
    "    def predict(self, sample, is_encoded=False):\n",
    "        self.net.eval()\n",
    "        \n",
    "        if is_encoded:\n",
    "            encoded = sample\n",
    "        else:\n",
    "            encoded = torch.from_numpy(self.encode_input(sample))\n",
    "\n",
    "        out = self.net(encoded)\n",
    "        return self.decode_output(out)\n",
    "    \n",
    "    def evaluate(self, testloader):\n",
    "        self.net.eval()\n",
    "        loss_list = []\n",
    "        n_correct = 0\n",
    "        n_fail = 0\n",
    "        for x, y_true, _, _ in testloader:\n",
    "            \n",
    "            y_pred = self.net(x)\n",
    "            loss_list.append(self.criterion(y_pred, y_true).data)\n",
    "            \n",
    "            if torch.argmax(y_pred).item() == y_true[0].item():\n",
    "                n_correct += 1\n",
    "            else:\n",
    "                n_fail += 1\n",
    "        \n",
    "        acc = n_correct / (n_correct + n_fail)\n",
    "        loss = np.mean([l.item() for l in loss_list])\n",
    "        print(f'Loss: {loss}, Acc: {acc}')\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, group, samples, labels, encoder, decoder):\n",
    "        \"\"\"\n",
    "            samples: list: ['sentence 1', 'sentence 2', ... 'sentence n']\n",
    "            labels: list: ['label 1', 'label 2', ... 'label n']\n",
    "            encoder: def for encoding input (in: <string> sentence, out: <np.ndarray> vec)\n",
    "            decoder: def for decoding output (in: <np.ndarray> vec, <string[]> labels, out: <string> label)\n",
    "        \"\"\"\n",
    "\n",
    "        self.group = group\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.samples_encoded = np.array([self.encoder(sample) for sample in self.samples])\n",
    "        self.sorted_labels = sorted(list(set(self.labels)))\n",
    "        self.target2label = {target:label for target, label in enumerate(self.sorted_labels)}\n",
    "        self.label2target = [self.sorted_labels.index(label) for label in self.labels]\n",
    "\n",
    "        self.x = torch.from_numpy(self.samples_encoded.reshape(-1, self.samples_encoded.shape[1]).astype('float32'))\n",
    "        self.y = torch.tensor(self.label2target)\n",
    "        self.y_one_hot = F.one_hot(self.y)\n",
    "        \n",
    "        self.p = self.x.shape[0]\n",
    "        self.n = len(self.x[0])\n",
    "        self.m = len(self.sorted_labels)\n",
    "\n",
    "        print(f'Dataset {self.group}: n = {self.n}, m = {self.m}, p = {self.p}')\n",
    "        print(f'Labels: {self.target2label}')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.samples[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.p\n",
    "    \n",
    "    def label(self, one_hot):\n",
    "        return self.decoder(one_hot, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2500 train samples, creating dataset...\n",
      "Dataset train: n = 768, m = 25, p = 2500\n",
      "Labels: {0: 'application_status', 1: 'change_language', 2: 'change_user_name', 3: 'definition', 4: 'find_phone', 5: 'flight_status', 6: 'flip_coin', 7: 'fun_fact', 8: 'improve_credit_score', 9: 'insurance_change', 10: 'maybe', 11: 'meaning_of_life', 12: 'oil_change_how', 13: 'payday', 14: 'pto_request', 15: 'replacement_card_duration', 16: 'restaurant_reservation', 17: 'shopping_list_update', 18: 'time', 19: 'timer', 20: 'transfer', 21: 'translate', 22: 'travel_alert', 23: 'what_can_i_ask_you', 24: 'where_are_you_from'}\n",
      "Loaded 500 dev samples, creating dataset...\n",
      "Dataset dev: n = 768, m = 25, p = 500\n",
      "Labels: {0: 'application_status', 1: 'change_language', 2: 'change_user_name', 3: 'definition', 4: 'find_phone', 5: 'flight_status', 6: 'flip_coin', 7: 'fun_fact', 8: 'improve_credit_score', 9: 'insurance_change', 10: 'maybe', 11: 'meaning_of_life', 12: 'oil_change_how', 13: 'payday', 14: 'pto_request', 15: 'replacement_card_duration', 16: 'restaurant_reservation', 17: 'shopping_list_update', 18: 'time', 19: 'timer', 20: 'transfer', 21: 'translate', 22: 'travel_alert', 23: 'what_can_i_ask_you', 24: 'where_are_you_from'}\n",
      "Loaded 750 test samples, creating dataset...\n",
      "Dataset test: n = 768, m = 25, p = 750\n",
      "Labels: {0: 'application_status', 1: 'change_language', 2: 'change_user_name', 3: 'definition', 4: 'find_phone', 5: 'flight_status', 6: 'flip_coin', 7: 'fun_fact', 8: 'improve_credit_score', 9: 'insurance_change', 10: 'maybe', 11: 'meaning_of_life', 12: 'oil_change_how', 13: 'payday', 14: 'pto_request', 15: 'replacement_card_duration', 16: 'restaurant_reservation', 17: 'shopping_list_update', 18: 'time', 19: 'timer', 20: 'transfer', 21: 'translate', 22: 'travel_alert', 23: 'what_can_i_ask_you', 24: 'where_are_you_from'}\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 50]          38,450\n",
      "            Linear-2                [-1, 1, 75]           3,825\n",
      "            Linear-3                [-1, 1, 25]           1,900\n",
      "================================================================\n",
      "Total params: 44,175\n",
      "Trainable params: 44,175\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "samples = {'train': [], 'dev': [], 'test': []}\n",
    "labels = {'train': [], 'dev': [], 'test': []}\n",
    "dataset = {}\n",
    "\n",
    "LIM = {'train': 2500, 'dev': 500, 'test': 750}\n",
    "\n",
    "for group in ('train', 'dev', 'test'):\n",
    "    with open(f'data/{group}-cs.tsv', 'r') as fr:\n",
    "        for line in fr.readlines()[:LIM[group]]:\n",
    "            sample, label = line.strip().split('\\t')\n",
    "            samples[group].append(sample)\n",
    "            labels[group].append(label)\n",
    "\n",
    "    print(f'Loaded {len(samples[group])} {group} samples, creating dataset...')\n",
    "    dataset[group] = IntentsDataset(\n",
    "        group=group,\n",
    "        samples=samples[group],\n",
    "        labels=labels[group],\n",
    "        encoder=model.encode_input,\n",
    "        decoder=model.decode_output\n",
    "    )\n",
    "    \n",
    "trainloader = DataLoader(dataset=dataset['train'], batch_size=NET_PARAMS['batch_size'], shuffle=True)\n",
    "devloader = DataLoader(dataset=dataset['dev'], batch_size=256, shuffle=True)\n",
    "testloader = DataLoader(dataset=dataset['test'], batch_size=1, shuffle=True)\n",
    "\n",
    "model.init_net(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, dev_loss tail: [0.2576027438044548, 0.25586526840925217, 0.25826171040534973, 0.25628622621297836, 0.2579463943839073, 0.257193461060524, 0.25787200778722763, 0.2554153949022293, 0.25691740959882736, 0.25600025802850723, 0.25364550203084946, 0.2544441893696785, 0.25601934641599655, 0.25734076648950577]\n",
      "Final train loss: 0.05671344064176083, dev loss: 0.26096343249082565\n",
      "Loss: 0.30482024207587044, Acc: 0.924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30482024207587044, 0.924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainloader, devloader, epochs=2000, verbose=True)\n",
    "model.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "\n",
    "while True:\n",
    "    inp = input('\\n>>')\n",
    "    if inp == 'stop':\n",
    "        break\n",
    "\n",
    "    print(f'>> {model.predict(inp)}')\n",
    "    print(model.net(torch.from_numpy(model.encode_input(inp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2500 train samples, creating dataset...\n",
      "Dataset train: n = 768, m = 5, p = 500\n",
      "Labels: {0: 'balance', 1: 'confirm_reservation', 2: 'freeze_account', 3: 'rollover_401k', 4: 'who_made_you'}\n",
      "Loaded 500 dev samples, creating dataset...\n",
      "Dataset dev: n = 768, m = 5, p = 100\n",
      "Labels: {0: 'balance', 1: 'confirm_reservation', 2: 'freeze_account', 3: 'rollover_401k', 4: 'who_made_you'}\n",
      "Loaded 750 test samples, creating dataset...\n",
      "Dataset test: n = 768, m = 5, p = 150\n",
      "Labels: {0: 'balance', 1: 'confirm_reservation', 2: 'freeze_account', 3: 'rollover_401k', 4: 'who_made_you'}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune\n",
    "\n",
    "samples_finetuning = {'train': [], 'dev': [], 'test': []}\n",
    "labels_finetuning = {'train': [], 'dev': [], 'test': []}\n",
    "dataset_finetuning = {}\n",
    "\n",
    "LIM1 = {'train': 2500, 'dev': 500, 'test': 750}\n",
    "LIM2 = {'train': 500, 'dev': 100, 'test': 150}\n",
    "\n",
    "for group in ('train', 'dev', 'test'):\n",
    "    with open(f'data/{group}-cs.tsv', 'r') as fr:\n",
    "        for line in fr.readlines()[LIM1[group]:LIM1[group]+LIM2[group]]:\n",
    "            sample, label = line.strip().split('\\t')\n",
    "            samples_finetuning[group].append(sample)\n",
    "            labels_finetuning[group].append(label)\n",
    "\n",
    "    print(f'Loaded {len(samples_finetuning[group])} {group} samples, creating dataset...')\n",
    "    dataset_finetuning[group] = IntentsDataset(\n",
    "        group=group,\n",
    "        samples=samples_finetuning[group],\n",
    "        labels=labels_finetuning[group],\n",
    "        encoder=model.encode_input,\n",
    "        decoder=model.decode_output\n",
    "    )\n",
    "    \n",
    "trainloader_finetuning = DataLoader(dataset=dataset_finetuning['train'], batch_size=NET_PARAMS['batch_size'], shuffle=True)\n",
    "devloader_finetuning = DataLoader(dataset=dataset_finetuning['dev'], batch_size=256, shuffle=True)\n",
    "testloader_finetuning = DataLoader(dataset=dataset_finetuning['test'], batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 50]          38,450\n",
      "            Linear-2                [-1, 1, 75]           3,825\n",
      "            Linear-3                 [-1, 1, 5]             380\n",
      "================================================================\n",
      "Total params: 42,655\n",
      "Trainable params: 42,655\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /Users/kitt/.cache/torch/sentence_transformers/fav-kky_FERNET-C5. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/kitt/.cache/torch/sentence_transformers/fav-kky_FERNET-C5 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 50]          38,450\n",
      "            Linear-2                [-1, 1, 75]           3,825\n",
      "            Linear-3                 [-1, 1, 5]             380\n",
      "================================================================\n",
      "Total params: 42,655\n",
      "Trainable params: 42,655\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Just replacing the last layer (depending on number of classes), otherwise keep trained weights (cool initialization)\n",
    "model.reinit_net(dataset=dataset_finetuning, keep_weights=True)\n",
    "\n",
    "# A new clean one for comparison\n",
    "model2 = Model()\n",
    "model2.init_net(dataset=dataset_finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, train loss 0.22241861559450626, dev loss 0.23995569348335266\n",
      "epoch 100, train loss 0.08063720818608999, dev loss 0.11053916811943054\n",
      "epoch 150, train loss 0.041384900687262416, dev loss 0.07387806475162506\n",
      "epoch 200, train loss 0.025776632828637958, dev loss 0.0577172189950943\n",
      "epoch 250, train loss 0.018183590611442924, dev loss 0.04846268519759178\n",
      "epoch 300, train loss 0.013555612764321268, dev loss 0.042365189641714096\n",
      "epoch 350, train loss 0.010889378434512764, dev loss 0.03825026750564575\n",
      "epoch 400, train loss 0.00896328070666641, dev loss 0.03508283197879791\n",
      "epoch 450, train loss 0.007517754449509084, dev loss 0.03263871371746063\n",
      "Epoch 500: Learning rate changed to 0.0225\n",
      "epoch 500, train loss 0.006497425143606961, dev loss 0.03066050074994564\n",
      "epoch 550, train loss 0.005895041336771101, dev loss 0.029417552053928375\n",
      "epoch 600, train loss 0.005353891116101295, dev loss 0.028338424861431122\n",
      "epoch 650, train loss 0.004928465816192329, dev loss 0.027391590178012848\n",
      "epoch 700, train loss 0.0046050717937760055, dev loss 0.026537757366895676\n",
      "epoch 750, train loss 0.004230541700962931, dev loss 0.02577245980501175\n",
      "epoch 800, train loss 0.0039575446280650795, dev loss 0.02507496066391468\n",
      "epoch 850, train loss 0.003728011273778975, dev loss 0.024438951164484024\n",
      "epoch 900, train loss 0.003488633257802576, dev loss 0.023860352113842964\n",
      "epoch 950, train loss 0.003300483134808019, dev loss 0.02335154451429844\n",
      "Epoch 1000: Learning rate changed to 0.016875\n",
      "epoch 1000, train loss 0.003113798942649737, dev loss 0.02285037934780121\n",
      "epoch 1050, train loss 0.002996597613673657, dev loss 0.022502968087792397\n",
      "epoch 1100, train loss 0.0028963523800484836, dev loss 0.022174514830112457\n",
      "epoch 1150, train loss 0.002779823582386598, dev loss 0.021865203976631165\n",
      "epoch 1200, train loss 0.002663637889781967, dev loss 0.02157878316938877\n",
      "epoch 1250, train loss 0.0025901968765538186, dev loss 0.02130254916846752\n",
      "epoch 1300, train loss 0.0025014543789438903, dev loss 0.02103945054113865\n",
      "epoch 1350, train loss 0.002416626492049545, dev loss 0.02078203484416008\n",
      "epoch 1400, train loss 0.0023520200193161145, dev loss 0.020537156611680984\n",
      "epoch 1450, train loss 0.002266134542878717, dev loss 0.020305095240473747\n",
      "Epoch 1500: Learning rate changed to 0.01265625\n",
      "epoch 1500, train loss 0.0022068127727834508, dev loss 0.020083578303456306\n",
      "epoch 1550, train loss 0.0021775063214590773, dev loss 0.01992429606616497\n",
      "epoch 1600, train loss 0.0021064149477751926, dev loss 0.01976446434855461\n",
      "epoch 1650, train loss 0.002069562950055115, dev loss 0.0196162611246109\n",
      "epoch 1700, train loss 0.0020305934158386663, dev loss 0.01947232335805893\n",
      "epoch 1750, train loss 0.0019867102819262072, dev loss 0.019329583272337914\n",
      "epoch 1800, train loss 0.0019343530730111524, dev loss 0.01919332891702652\n",
      "epoch 1850, train loss 0.0019081226782873273, dev loss 0.019055364653468132\n",
      "epoch 1900, train loss 0.001858161631389521, dev loss 0.018927155062556267\n",
      "epoch 1950, train loss 0.001829509186791256, dev loss 0.018797200173139572\n",
      "Epoch 2000: Learning rate changed to 0.0094921875\n",
      "epoch 2000, train loss 0.0017937618831638247, dev loss 0.018669134005904198\n",
      "Final train loss: 0.0017937618831638247, dev loss: 0.018669134005904198\n",
      "Loss: 0.06950717480940512, Acc: 0.98\n",
      "epoch 50, train loss 1.5047967731952667, dev loss 1.5014714002609253\n",
      "epoch 100, train loss 0.8267327100038528, dev loss 0.8263451457023621\n",
      "epoch 150, train loss 0.27113002352416515, dev loss 0.2895260155200958\n",
      "epoch 200, train loss 0.11177240964025259, dev loss 0.14200831949710846\n",
      "epoch 250, train loss 0.058489935006946325, dev loss 0.09285454452037811\n",
      "epoch 300, train loss 0.037284637335687876, dev loss 0.07029727101325989\n",
      "epoch 350, train loss 0.02597431861795485, dev loss 0.0577993169426918\n",
      "epoch 400, train loss 0.01959687704220414, dev loss 0.05003972351551056\n",
      "epoch 450, train loss 0.01558378676418215, dev loss 0.04472913593053818\n",
      "Epoch 500: Learning rate changed to 0.0225\n",
      "epoch 500, train loss 0.012760271085426211, dev loss 0.040884796530008316\n",
      "epoch 550, train loss 0.01118956075515598, dev loss 0.038596149533987045\n",
      "epoch 600, train loss 0.009944436489604414, dev loss 0.03673684224486351\n",
      "epoch 650, train loss 0.008999907528050244, dev loss 0.035143207758665085\n",
      "epoch 700, train loss 0.008145112078636885, dev loss 0.03381216153502464\n",
      "epoch 750, train loss 0.007436979736667126, dev loss 0.032677069306373596\n",
      "epoch 800, train loss 0.0068008400849066675, dev loss 0.031678035855293274\n",
      "epoch 850, train loss 0.00627483957214281, dev loss 0.030774464830756187\n",
      "epoch 900, train loss 0.005858207296114415, dev loss 0.030035706236958504\n",
      "epoch 950, train loss 0.005464575428050011, dev loss 0.029335081577301025\n",
      "Epoch 1000: Learning rate changed to 0.016875\n",
      "epoch 1000, train loss 0.005113918683491647, dev loss 0.028718359768390656\n",
      "epoch 1050, train loss 0.00483907712623477, dev loss 0.028271345421671867\n",
      "epoch 1100, train loss 0.004653720825444907, dev loss 0.02787754312157631\n",
      "epoch 1150, train loss 0.004422088240971789, dev loss 0.027523033320903778\n",
      "epoch 1200, train loss 0.004269539465894923, dev loss 0.027184076607227325\n",
      "epoch 1250, train loss 0.004069707938469946, dev loss 0.02685544267296791\n",
      "epoch 1300, train loss 0.003918809728929773, dev loss 0.02656383253633976\n",
      "epoch 1350, train loss 0.0037790033675264567, dev loss 0.02627602592110634\n",
      "epoch 1400, train loss 0.0036360117373988032, dev loss 0.02599862962961197\n",
      "epoch 1450, train loss 0.0035029686987400055, dev loss 0.025731481611728668\n",
      "Epoch 1500: Learning rate changed to 0.01265625\n",
      "epoch 1500, train loss 0.003380894981091842, dev loss 0.025503790006041527\n",
      "epoch 1550, train loss 0.003314130357466638, dev loss 0.025335386395454407\n",
      "epoch 1600, train loss 0.0032270069932565093, dev loss 0.02518121525645256\n",
      "epoch 1650, train loss 0.0031508271058555692, dev loss 0.025022517889738083\n",
      "epoch 1700, train loss 0.0030580639140680432, dev loss 0.02487393468618393\n",
      "epoch 1750, train loss 0.003003481775522232, dev loss 0.024709079414606094\n",
      "epoch 1800, train loss 0.002926952118286863, dev loss 0.02456432394683361\n",
      "epoch 1850, train loss 0.002862919995095581, dev loss 0.02444015070796013\n",
      "epoch 1900, train loss 0.00280743750045076, dev loss 0.024317722767591476\n",
      "epoch 1950, train loss 0.0027378613594919443, dev loss 0.024190343916416168\n",
      "Epoch 2000: Learning rate changed to 0.0094921875\n",
      "epoch 2000, train loss 0.00270133392768912, dev loss 0.024067796766757965\n",
      "Final train loss: 0.00270133392768912, dev loss: 0.024067796766757965\n",
      "Loss: 0.04942478734922285, Acc: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04942478734922285, 0.98)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetuning\n",
    "train_loss_ft, dev_loss_ft = model.fit(trainloader_finetuning, devloader_finetuning, epochs=2000, verbose=True)\n",
    "model.evaluate(testloader_finetuning)\n",
    "\n",
    "# from scratch\n",
    "train_loss_scratch, dev_loss_scratch = model2.fit(trainloader_finetuning, devloader_finetuning, epochs=2000, verbose=True)\n",
    "model2.evaluate(testloader_finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(dev_loss_scratch[:500], label='training from scratch')\n",
    "plt.plot(dev_loss_ft[:500], label='fine-tuning')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('dev loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
